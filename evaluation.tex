\chapter{Evaluation}
\label{evaluation}
By testing this new system on users who are also using the original system it should be possible to compare the two systems and measure how effective the new widget has been. Volunteers from the currently running course will be asked to use this system instead of the live system for certain weeks. Variations of this system may also be tested to see if different visual designs result in different progress.

\section{Objective Measures}
With users switching between systems and not every week being captured objective measures be less effective. Average score, number attempts and time taken to submit exercises can still be compared but careful consideration will need to be given to ensure accurate conclusions are drawn.

\section{User Feedback}
Users will be given questionnaires to complete for each system. The questionnaires will contain question asking about general use of the system, and if the widget helped their learning in any way. Here users can explain some designs were particularly motivating or if they had multiple interpretations.

\section{Technical Difficulties}
Since these tests are running alongside the course as it is running there are some things which must be taken into consideration. Since learning is being measured, the user must use this system, for a given week, before the user has used the other system. Otherwise, the user will have already done the exercises. This means the files the user submits to this system must be transferred to the live system as well since it is unlikely the user would be willing to do all the exercises twice. The solution to this problem is yet to be decided but will most likely be asking the user to resubmit all their exercises in the live system at the end of the week. Efforts will be coordinated with the organisers of the course to limit problems for the user.
